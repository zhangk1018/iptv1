import os
import re
import time
import requests
import concurrent.futures
import sys

# ===============================
# é…ç½®åŒº
# ===============================
INPUT_FILES = [
    "live.txt",
    "IPTV.txt"   
]
OUTPUT_FILE = "livezubo.txt"
CHECK_COUNT = 2
TEST_DURATION = 12

# ä¸¥æ ¼æ¨¡å¼ï¼ˆæ¨èä¸»åŠ›ï¼‰
MIN_PEAK_REQUIRED   = 1.00
MIN_STABLE_REQUIRED = 0.90   # â† è°·åº•å‚è€ƒæ˜¯å…³é”®ï¼Œ0.9+ æ‰çœŸæ­£ç¨³

# é™çº§æ¨¡å¼ï¼ˆè‡ªåŠ¨è§¦å‘æ—¶ç”¨ï¼‰
FALLBACK_PEAK   = 0.95
FALLBACK_STABLE = 0.75

def get_realtime_speed(url):
    """è¿”å›ï¼šå³°å€¼é€Ÿåº¦, ååŠæ®µå¹³å‡é€Ÿåº¦(è°·åº•å‚è€ƒ), æ•´ä½“å¹³å‡é€Ÿåº¦"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) PotPlayer/23.9.22",
        "Accept": "*/*"
    }
    try:
        start_time = time.time()
        total_size = 0
        speed_samples = []
        last_size = 0
        last_check = start_time

        with requests.get(url, stream=True, timeout=15, headers=headers) as r:
            if r.status_code != 200:
                return 0.0, 0.0, 0.0

            for chunk in r.iter_content(chunk_size=1024*256):
                if chunk:
                    total_size += len(chunk)
                    now = time.time()

                    if now - last_check >= 0.8:
                        interval = now - last_check
                        current_speed = (total_size - last_size) / interval / 1024 / 1024
                        speed_samples.append(current_speed)
                        last_size = total_size
                        last_check = now

                    if now - start_time > TEST_DURATION:
                        break

        duration = time.time() - start_time
        if duration < 3 or total_size == 0:
            return 0.0, 0.0, 0.0

        overall_avg = (total_size / 1024 / 1024) / duration

        if not speed_samples:
            return overall_avg, overall_avg, overall_avg

        peak_speed = max(speed_samples)
        split_idx = max(2, len(speed_samples) * 4 // 10)
        stable_avg = sum(speed_samples[split_idx:]) / len(speed_samples[split_idx:]) if len(speed_samples) > 4 else overall_avg

        return peak_speed, stable_avg, overall_avg

    except Exception:
        return 0.0, 0.0, 0.0


import random  # â† è®°å¾—åœ¨è„šæœ¬é¡¶éƒ¨æ·»åŠ è¿™ä¸ªå¯¼å…¥

def test_ip_group(ip_port, channels):
    # ä¼˜å…ˆåŒ¹é… CCTV-4 / æ¹–å—å«è§† çš„å¸¸è§å†™æ³•ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå…¼å®¹å„ç§åˆ«åï¼‰
    keywords = [
        "CCTV4", "CCTV-4", "CCTV-04", "CCTV4ä¸­æ–‡å›½é™…", "CCTV-4ä¸­æ–‡å›½é™…", "ä¸­æ–‡å›½é™…", "CCTV4å›½é™…",
        "æ¹–å—å«è§†", "æ¹–å—", "HUNAN", "å¿«ä¹å¤§æœ¬è¥", "èŠ’æœ"  # èŠ’æœTVç›¸å…³æœ‰æ—¶ä¼šå¸¦
    ]
    
    test_targets = []
    for name, url in channels:
        upper_name = name.upper()
        if any(kw.upper() in upper_name for kw in keywords):
            test_targets.append(url)
    
    # å¦‚æœæ‰¾åˆ°çš„ >= CHECK_COUNTï¼ˆé»˜è®¤2ï¼‰ï¼Œå°±å–å‰å‡ ä¸ª
    if len(test_targets) >= CHECK_COUNT:
        test_targets = test_targets[:CHECK_COUNT]
    
    # å¦‚æœä¸å¤Ÿæˆ–å®Œå…¨æ²¡æ‰¾åˆ°ï¼Œå°±éšæœºè¡¥é½/å…¨éšæœº
    else:
        remaining = CHECK_COUNT - len(test_targets)
        other_channels = [url for n, url in channels if url not in test_targets]
        
        if other_channels:
            # éšæœºé€‰ remaining ä¸ªä¸é‡å¤çš„
            random_selected = random.sample(other_channels, min(remaining, len(other_channels)))
            test_targets.extend(random_selected)
        else:
            # æç«¯æƒ…å†µï¼šæœåŠ¡å™¨åªæœ‰ä¸€ä¸ªé¢‘é“ï¼Œå°±å…¨ç”¨å®ƒ
            test_targets = [url for _, url in channels][:CHECK_COUNT]
    
    # å¦‚æœè¿˜æ˜¯ç©ºï¼ˆä¸å¯èƒ½ï¼Œä½†é˜²é”™ï¼‰ï¼Œå°±è·³è¿‡æˆ–ç”¨ç¬¬ä¸€ä¸ª
    if not test_targets:
        test_targets = [channels[0][1]] if channels else []
    
    # ä¸‹é¢ç»§ç»­åŸæ¥çš„æµ‹è¯•é€»è¾‘...
    best_peak = 0.0
    best_stable = 0.0
    best_overall = 0.0
    best_url = ""
    
    for url in test_targets:
        peak, stable, overall = get_realtime_speed(url)
        # ä¼˜å…ˆå³°å€¼ï¼Œå…¶æ¬¡ç¨³å®šæ€§
        if (peak > best_peak) or (peak == best_peak and stable > best_stable):
            best_peak = peak
            best_stable = stable
            best_overall = overall
            best_url = url
    
    timestamp = time.strftime("%H:%M:%S", time.localtime())
    sys.stdout.write(
        f"[{timestamp}] {ip_port:21} â†’ "
        f"å³°å€¼:{best_peak:5.2f}  è°·åº•å‚è€ƒ:{best_stable:5.2f}  æ•´ä½“:{best_overall:5.2f} MB/s   æµ‹è¯•ç”¨: {best_url[:70]}\n"
    )
    sys.stdout.flush()
    
    return ip_port, best_peak, best_stable, best_overall


# ... å…¶ä»–é…ç½®ä¸å˜

def main():
    if not INPUT_FILES:
        print("âš ï¸ æ²¡æœ‰é…ç½®ä»»ä½•è¾“å…¥æ–‡ä»¶ï¼Œè„šæœ¬é€€å‡ºã€‚")
        return

    all_lines = []
    for input_file in INPUT_FILES:
        if not os.path.exists(input_file):
            print(f"âš ï¸ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue
        with open(input_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            all_lines.extend(lines)
        print(f"å·²è¯»å– {input_file}ï¼Œå…± {len(lines)} è¡Œ")

    # ç»§ç»­åŸæ¥çš„è§£æé€»è¾‘ï¼Œä½¿ç”¨ all_lines ä»£æ›¿ lines
    ip_groups = {}
    other_info = []
    for line in all_lines:
        line = line.strip()
        if "," in line and "$" in line:
            name, url_part = line.split(",", 1)
            match = re.search(r'http://(.*?)/', url_part)
            if match:
                ip_port = match.group(1)
                ip_groups.setdefault(ip_port, []).append((name, url_part))
        elif line:
            other_info.append(line)

    # åé¢ä»£ç å®Œå…¨ä¸å˜...

    print(f"\nğŸš€ å¯åŠ¨ç­›é€‰ | å€™é€‰æœåŠ¡å™¨: {len(ip_groups)} ä¸ª | æµ‹è¯•æ—¶é•¿: {TEST_DURATION}s\n")

    results = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(test_ip_group, ip, chs): ip for ip, chs in ip_groups.items()}
        for future in concurrent.futures.as_completed(futures):
            ip_port, peak, stable, overall = future.result()
            results[ip_port] = (peak, stable, overall)

    print("\n" + "="*70)

    # ç­›é€‰æœåŠ¡å™¨
    selected_ips = [
        ip for ip, (peak, stable, _) in results.items()
        if peak >= MIN_PEAK_REQUIRED and stable >= MIN_STABLE_REQUIRED
    ]

    final_step = f"å³°å€¼â‰¥{MIN_PEAK_REQUIRED} & è°·åº•â‰¥{MIN_STABLE_REQUIRED}"

    if not selected_ips:
        print("âŒ æ²¡æœ‰å®Œå…¨è¾¾æ ‡æœåŠ¡å™¨ï¼Œè¿›å…¥é™çº§æ¨¡å¼...")
        selected_ips = [
            ip for ip, (peak, stable, _) in results.items()
            if peak >= FALLBACK_PEAK and stable >= FALLBACK_STABLE
        ]
        final_step = f"é™çº§æ¨¡å¼ï¼šå³°å€¼â‰¥{FALLBACK_PEAK} & è°·åº•â‰¥{FALLBACK_STABLE}"

    if not selected_ips:
        best_ip = max(results, key=lambda x: results[x][0])
        selected_ips = [best_ip]
        peak, stable, _ = results[best_ip]
        final_step = f"ä¿åº•æœ€å¿«ï¼šå³°å€¼ {peak:.2f} / è°·åº• {stable:.2f}"

    print(f"âœ… æœ€ç»ˆå…¥é€‰ {len(selected_ips)} ä¸ªæœåŠ¡å™¨ï¼ˆæ ‡å‡†ï¼š{final_step}ï¼‰\n")

    # ===================== è¾“å‡ºéƒ¨åˆ† - å¢åŠ å»é‡ =====================
    try:
        from fofa_fetch import CHANNEL_CATEGORIES
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥ CHANNEL_CATEGORIESï¼Œè¯·æ£€æŸ¥ fofa_fetch.py")
        return

    final_output = [l for l in other_info if "#genre#" in l or "æ›´æ–°æ—¶é—´" in l]
    final_output.append("")

    for category, ch_list in CHANNEL_CATEGORIES.items():
        category_added = False

        for std_name in ch_list:
            # ç”¨ dict å­˜å‚¨ url â†’ (peak, stable) ï¼Œå¤©ç„¶å»é‡
            url_info = {}

            for ip in selected_ips:
                for name, url_part in ip_groups.get(ip, []):
                    if name == std_name:
                        peak, stable, _ = results[ip]
                        # å¦‚æœå·²æœ‰ç›¸åŒurlï¼Œå–æ›´å¥½çš„è¯„åˆ†
                        if url_part not in url_info or (peak, stable) > url_info[url_part]:
                            url_info[url_part] = (peak, stable)

            if not url_info:
                continue

            # æŒ‰ (å³°å€¼, è°·åº•) é™åºæ’åº
            sorted_entries = sorted(
                url_info.items(),
                key=lambda x: (x[1][0], x[1][1]),
                reverse=True
            )

            # åªå–æœ€å¥½çš„é‚£ä¸€ä¸ªï¼ˆå·²å»é‡ï¼‰
            if sorted_entries:
                if not category_added:
                    final_output.append(f"{category},#genre#")
                    category_added = True
                
                best_url, _ = sorted_entries[0]
                final_output.append(f"{std_name},{best_url}")

        if category_added:
            final_output.append("")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(final_output).rstrip() + "\n")
    # ...ï¼ˆå‰é¢çš„ for category å¾ªç¯ä¸å˜ï¼Œæ”¶é›† final_outputï¼‰

    # ===================== æœ€ç»ˆå…¨å±€å»é‡ =====================
    seen_lines = set()  # ç”¨æ•´è¡Œå†…å®¹å»é‡ï¼ˆæœ€ä¸¥æ ¼ï¼Œé€‚åˆä½ æè¿°çš„é‡å¤æ•´è¡Œæƒ…å†µï¼‰
    unique_output = []

    for line in final_output:
        stripped = line.strip()
        if not stripped:  # ç©ºè¡Œä¿ç•™
            unique_output.append(line)
            continue

        # ä¿ç•™åˆ†ç±»æ ‡é¢˜ã€å¤´éƒ¨ä¿¡æ¯ï¼ˆå³ä½¿é‡å¤ä¹Ÿæ— æ‰€è°“ï¼Œé€šå¸¸ä¸ä¼šé‡å¤ï¼‰
        if ",#genre#" in stripped or "æ›´æ–°æ—¶é—´" in stripped:
            unique_output.append(line)
            continue

        # é¢‘é“è¡Œï¼šåªæ·»åŠ æ²¡è§è¿‡çš„
        if stripped not in seen_lines:
            seen_lines.add(stripped)
            unique_output.append(line)

    # åªå†™ä¸€æ¬¡æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(unique_output).rstrip() + "\n")

    print(f"\nğŸ¯ ç­›é€‰å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_FILE}")
    print(f"   å·²ä¿ç•™ {len(selected_ips)} ä¸ªæœåŠ¡å™¨æºï¼Œå…¨å±€å»é‡åæ— é‡å¤è¡Œ")

if __name__ == "__main__":
    main()
